{
 "cells": [
  {
   "cell_type": "code",
   "id": "9f0834eb1605c2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T14:57:45.965245Z",
     "start_time": "2024-11-25T14:57:30.446126Z"
    }
   },
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I551385/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/accelerate/utils/other.py:220: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  np.core.multiarray._reconstruct,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b94177ac494c4d0da898f8a374b51f47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:22:35.478097Z",
     "start_time": "2024-11-27T12:22:35.394504Z"
    }
   },
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "#client = weaviate.connect_to_custom(http_host=os.getenv(\"WEAVIATE_HTTP_HOST\"),http_port=int(os.getenv(\"WEAVIATE_HTTP_PORT\")), http_secure=True, grpc_host=os.getenv(\"WEAVIATE_GRPC_HOST\"), grpc_port=int(os.getenv(\"WEAVIATE_GRPC_PORT\")), grpc_secure=True, auth_credentials=weaviate.auth.AuthApiKey(api_key=os.getenv(\"WEAVIATE_API_KEY\")))\n",
    "client = weaviate.connect_to_local()"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "f5a24c0a420b473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:23:03.575271Z",
     "start_time": "2024-11-27T12:23:03.516947Z"
    }
   },
   "source": "#client.collections.delete(\"neuclir_1_mutli_bge_m3\")",
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "id": "ad1e0841b2063e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:23:07.272220Z",
     "start_time": "2024-11-27T12:23:07.103823Z"
    }
   },
   "source": [
    "try:\n",
    "    collection = client.collections.get(\"neuclir_1_mutli_bge_m3\")\n",
    "    documents = client.collections.create(\n",
    "        name=\"neuclir_1_mutli_bge_m3\",\n",
    "        vectorizer_config=[\n",
    "            wvc.config.Configure.NamedVectors.none(\n",
    "                name=\"title_dense\",\n",
    "                vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "                    vector_cache_max_objects=100000,\n",
    "                    #quantizer=wvc.config.Reconfigure.VectorIndex.Quantizer.pq(training_limit=100000)  # Set the threshold to begin training\n",
    "                ),\n",
    "                \n",
    "            ),\n",
    "            wvc.config.Configure.NamedVectors.none( \n",
    "                name=\"text_dense\",\n",
    "                vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "                    vector_cache_max_objects=100000,\n",
    "                   # quantizer=wvc.config.Reconfigure.VectorIndex.Quantizer.pq(training_limit=10000)  # Set the threshold to begin training\n",
    "                ),\n",
    "            )],\n",
    "        properties=[\n",
    "            wvc.config.Property(\n",
    "                name=\"doc_id\",\n",
    "                data_type=wvc.config.DataType.UUID,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"title_sparse\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "              wvc.config.Property(\n",
    "                name=\"document_sparse\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"title_colbert\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "           wvc.config.Property(\n",
    "                name=\"document_colbert\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"title\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"text\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"url\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            )\n",
    "        ])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "823c74b126a2840e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T01:15:42.687406Z",
     "start_time": "2024-11-25T01:13:24.741656Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from explore.funcs import load_datasets\n",
    "\n",
    "datasets = load_datasets([\"zh\", \"ru\", \"fa\"])\n",
    "\n",
    "docs = pd.concat([pd.DataFrame(dataset.docs_iter()) for dataset in datasets.values()])\n",
    "\n",
    "\n",
    "len(docs)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1876367"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662db154dede72e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T00:54:59.549962Z",
     "start_time": "2024-11-25T00:54:59.546918Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "def to_blob(obj):\n",
    "    return base64.b64encode(pickle.dumps(obj)).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "id": "e58458bf4fd4ff63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T02:03:29.326892Z",
     "start_time": "2024-11-25T01:59:18.271081Z"
    }
   },
   "source": [
    "import pickle\n",
    "\n",
    "batches = [docs[i:i+10000] for i in range(100000, len(docs), 10000)]\n",
    "zh = client.collections.get(\"hc4_filtered_bge_m3\")\n",
    "\n",
    "outer_progress = tqdm(total=len(docs))\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    title_embeddings = model.encode(batch[\"title\"].to_list(), return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "    doc_embeddings = model.encode(batch[\"text\"].to_list(), return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "    #title_sparse_blobs = [to_blob(x) for x in title_embeddings[\"lexical_weights\"]]\n",
    "    #title_colbert_blobs = [to_blob(x) for x in title_embeddings[\"colbert_vecs\"]]\n",
    "    batch = batch.reset_index(drop=True)\n",
    "    with zh.batch.fixed_size(100, 2) as b:\n",
    "        for row in batch.itertuples(index=True):\n",
    "            b.add_object(properties={\n",
    "                \"doc_id\": row.doc_id,\n",
    "                \"title\": row.title,\n",
    "                \"text\": row.text,\n",
    "                \"url\": row.url\n",
    "                #\"title_sparse\": title_sparse_blobs[row.Index],\n",
    "                #\"title_colbert\": title_colbert_blobs[row.Index],\n",
    "            }, vector={  \n",
    "                \"title_dense\": title_embeddings[\"dense_vecs\"][row.Index],\n",
    "                \"text_dense\": doc_embeddings[\"dense_vecs\"][row.Index]\n",
    "            }, uuid=row.doc_id)\n",
    "            outer_progress.update(1)\n",
    "        if b.number_errors != 0:\n",
    "            print(f\"Found Errors: {b.number_errors}\")\n",
    "        b.flush()\n",
    "        "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1876367 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7915fb97cfe5412f8b6f515f4f5cabf7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Inference Embeddings:   0%|          | 0/40 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:   2%|▎         | 1/40 [00:01<01:16,  1.97s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:   5%|▌         | 2/40 [00:02<00:52,  1.38s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:   8%|▊         | 3/40 [00:03<00:43,  1.17s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  10%|█         | 4/40 [00:04<00:37,  1.05s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  12%|█▎        | 5/40 [00:05<00:34,  1.02it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  15%|█▌        | 6/40 [00:06<00:31,  1.07it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  18%|█▊        | 7/40 [00:07<00:29,  1.13it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  20%|██        | 8/40 [00:08<00:27,  1.15it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  22%|██▎       | 9/40 [00:08<00:26,  1.19it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  25%|██▌       | 10/40 [00:09<00:24,  1.21it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  28%|██▊       | 11/40 [00:10<00:23,  1.24it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  30%|███       | 12/40 [00:11<00:21,  1.28it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  32%|███▎      | 13/40 [00:11<00:20,  1.31it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  35%|███▌      | 14/40 [00:12<00:19,  1.33it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  38%|███▊      | 15/40 [00:13<00:18,  1.36it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  40%|████      | 16/40 [00:13<00:17,  1.39it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  42%|████▎     | 17/40 [00:14<00:16,  1.40it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  45%|████▌     | 18/40 [00:15<00:15,  1.42it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  48%|████▊     | 19/40 [00:15<00:14,  1.43it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  50%|█████     | 20/40 [00:16<00:13,  1.43it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  52%|█████▎    | 21/40 [00:17<00:12,  1.46it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  55%|█████▌    | 22/40 [00:17<00:12,  1.50it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  57%|█████▊    | 23/40 [00:18<00:11,  1.51it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  60%|██████    | 24/40 [00:19<00:10,  1.53it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  62%|██████▎   | 25/40 [00:19<00:09,  1.56it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  65%|██████▌   | 26/40 [00:20<00:08,  1.59it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  68%|██████▊   | 27/40 [00:21<00:07,  1.63it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  70%|███████   | 28/40 [00:21<00:07,  1.67it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  72%|███████▎  | 29/40 [00:22<00:06,  1.70it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  75%|███████▌  | 30/40 [00:22<00:05,  1.69it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  78%|███████▊  | 31/40 [00:23<00:05,  1.70it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  80%|████████  | 32/40 [00:23<00:04,  1.70it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  82%|████████▎ | 33/40 [00:24<00:04,  1.74it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  85%|████████▌ | 34/40 [00:25<00:03,  1.77it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  88%|████████▊ | 35/40 [00:25<00:02,  1.80it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  90%|█████████ | 36/40 [00:26<00:02,  1.85it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  92%|█████████▎| 37/40 [00:26<00:01,  1.89it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  95%|█████████▌| 38/40 [00:27<00:01,  1.97it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings: 100%|██████████| 40/40 [00:27<00:00,  1.46it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Inference Embeddings:   0%|          | 0/40 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:   2%|▎         | 1/40 [00:23<15:09, 23.31s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:   5%|▌         | 2/40 [00:43<13:37, 21.51s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:   8%|▊         | 3/40 [01:03<12:54, 20.92s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  10%|█         | 4/40 [01:24<12:23, 20.66s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  12%|█▎        | 5/40 [01:44<12:03, 20.66s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  15%|█▌        | 6/40 [02:04<11:37, 20.52s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  18%|█▊        | 7/40 [02:25<11:15, 20.46s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  20%|██        | 8/40 [02:45<10:54, 20.45s/it]\u001B[A\u001B[A\n",
      "\n",
      "Inference Embeddings:  22%|██▎       | 9/40 [03:26<11:51, 22.95s/it]\u001B[A\u001B[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batches):\n\u001B[1;32m      9\u001B[0m     title_embeddings \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mencode(batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto_list(), return_dense\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, return_colbert_vecs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 10\u001B[0m     doc_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dense\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_colbert_vecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m#title_sparse_blobs = [to_blob(x) for x in title_embeddings[\"lexical_weights\"]]\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m#title_colbert_blobs = [to_blob(x) for x in title_embeddings[\"colbert_vecs\"]]\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     batch \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:207\u001B[0m, in \u001B[0;36mM3Embedder.encode\u001B[0;34m(self, queries, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, **kwargs)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_sparse \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: return_sparse \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_sparse\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_colbert_vecs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: return_colbert_vecs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_colbert_vecs\n\u001B[0;32m--> 207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dense\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dense\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_colbert_vecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_colbert_vecs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py:237\u001B[0m, in \u001B[0;36mAbsEmbedder.encode\u001B[0;34m(self, sentences, batch_size, max_length, convert_to_numpy, instruction, instruction_format, **kwargs)\u001B[0m\n\u001B[1;32m    233\u001B[0m         sentences \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_detailed_instruct(instruction_format, instruction, sentence) \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m\n\u001B[1;32m    234\u001B[0m                      sentences]\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sentences, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_devices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_single_device\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m        \u001B[49m\u001B[43msentences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_devices\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_multi_process_pool(AbsEmbedder\u001B[38;5;241m.\u001B[39m_encode_multi_process_worker)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:331\u001B[0m, in \u001B[0;36mM3Embedder.encode_single_device\u001B[0;34m(self, sentences, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, device, **kwargs)\u001B[0m\n\u001B[1;32m    323\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m    324\u001B[0m     inputs_batch,\n\u001B[1;32m    325\u001B[0m     return_dense\u001B[38;5;241m=\u001B[39mreturn_dense,\n\u001B[1;32m    326\u001B[0m     return_sparse\u001B[38;5;241m=\u001B[39mreturn_sparse,\n\u001B[1;32m    327\u001B[0m     return_colbert_vecs\u001B[38;5;241m=\u001B[39mreturn_colbert_vecs\n\u001B[1;32m    328\u001B[0m )\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dense:\n\u001B[0;32m--> 331\u001B[0m     all_dense_embeddings\u001B[38;5;241m.\u001B[39mappend(\u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdense_vecs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_sparse:\n\u001B[1;32m    334\u001B[0m     token_weights \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_vecs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "226c41cad9bfe2c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:32:17.556819Z",
     "start_time": "2024-11-27T14:32:17.506802Z"
    }
   },
   "source": [
    "\n",
    "zh = client.collections.get(\"neuclir_1_mutli_bge_m3\")\n",
    "aggregation = zh.aggregate.over_all(total_count=True)\n",
    "print(aggregation.total_count)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487620\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:32:14.669254Z",
     "start_time": "2024-11-27T14:32:14.542575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.cluster.nodes(\n",
    "    collection=\"neuclir_1_mutli_bge_m3\",\n",
    "    output=\"verbose\"\n",
    ")"
   ],
   "id": "90332a5b350cecd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(git_hash='cfdbdd0', name='node1', shards=[Shard(collection='Neuclir_1_mutli_bge_m3', name='xhHerVp3Eq14', node='node1', object_count=487300, vector_indexing_status='READY', vector_queue_length=0, compressed=False, loaded=True)], stats=Stats(object_count=487300, shard_count=1), status='HEALTHY', version='1.27.5')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "id": "7ab8d7371f8274a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T14:57:49.871109Z",
     "start_time": "2024-11-25T14:57:45.971976Z"
    }
   },
   "source": [
    "from weaviate.collections.classes.grpc import MetadataQuery\n",
    "\n",
    "query_embeddings = model.encode([\"What political impact do news have on teenagers?\"], return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "response = zh.query.near_vector(near_vector=query_embeddings[\"dense_vecs\"][0], target_vector=\"title_dense\", limit=50, return_metadata=MetadataQuery(distance=True))\n",
    "\n",
    "for o in response.objects:\n",
    "    print(o.metadata.distance)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3497281074523926\n",
      "0.3542219400405884\n",
      "0.36670786142349243\n",
      "0.3710690140724182\n",
      "0.3710908889770508\n",
      "0.379378080368042\n",
      "0.38135790824890137\n",
      "0.3862995505332947\n",
      "0.38806724548339844\n",
      "0.39258015155792236\n",
      "0.394734263420105\n",
      "0.39501863718032837\n",
      "0.39628756046295166\n",
      "0.3965839147567749\n",
      "0.3984275460243225\n",
      "0.39956390857696533\n",
      "0.399910032749176\n",
      "0.4027395248413086\n",
      "0.40353405475616455\n",
      "0.40462398529052734\n",
      "0.406577467918396\n",
      "0.40823471546173096\n",
      "0.41130876541137695\n",
      "0.4132359027862549\n",
      "0.41399049758911133\n",
      "0.41496437788009644\n",
      "0.41694867610931396\n",
      "0.4179171919822693\n",
      "0.4182232618331909\n",
      "0.4188825488090515\n",
      "0.419994592666626\n",
      "0.42144525051116943\n",
      "0.4215751886367798\n",
      "0.42173147201538086\n",
      "0.4217910170555115\n",
      "0.4219786524772644\n",
      "0.4228867292404175\n",
      "0.42291176319122314\n",
      "0.42298340797424316\n",
      "0.42474257946014404\n",
      "0.425207257270813\n",
      "0.42567533254623413\n",
      "0.4257087707519531\n",
      "0.42625755071640015\n",
      "0.426602840423584\n",
      "0.42671459913253784\n",
      "0.4272133708000183\n",
      "0.42755210399627686\n",
      "0.4275628328323364\n",
      "0.4281851649284363\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a829c-4ef3-4109-bffd-ede0877d812f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
