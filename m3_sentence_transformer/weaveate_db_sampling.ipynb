{
 "cells": [
  {
   "cell_type": "code",
   "id": "9f0834eb1605c2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T16:15:39.735699Z",
     "start_time": "2024-11-27T16:15:30.632620Z"
    }
   },
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from m3_sentence_transformer.data_sampler import get_sample_docs_with_all_qrels\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a68d46273ba43d1b1a3934c19d02c10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:20:24.687917Z",
     "start_time": "2024-11-27T18:20:24.579348Z"
    }
   },
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "#client = weaviate.connect_to_custom(http_host=os.getenv(\"WEAVIATE_HTTP_HOST\"),http_port=int(os.getenv(\"WEAVIATE_HTTP_PORT\")), http_secure=True, grpc_host=os.getenv(\"WEAVIATE_GRPC_HOST\"), grpc_port=int(os.getenv(\"WEAVIATE_GRPC_PORT\")), grpc_secure=True, auth_credentials=weaviate.auth.AuthApiKey(api_key=os.getenv(\"WEAVIATE_API_KEY\")))\n",
    "client = weaviate.connect_to_local()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f5a24c0a420b473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:20:31.753610Z",
     "start_time": "2024-11-27T18:20:31.593409Z"
    }
   },
   "source": "#client.collections.delete(\"neuclir_1_mutli_bge_m3_small\")",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "ad1e0841b2063e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:20:32.682089Z",
     "start_time": "2024-11-27T18:20:32.506927Z"
    }
   },
   "source": [
    "try:\n",
    "    documents = client.collections.create(\n",
    "        name=\"neuclir_1_mutli_bge_m3_small\",\n",
    "        vectorizer_config=[\n",
    "            wvc.config.Configure.NamedVectors.none(\n",
    "                name=\"title_dense\",\n",
    "                vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "                    vector_cache_max_objects=100000,\n",
    "                    #quantizer=wvc.config.Reconfigure.VectorIndex.Quantizer.pq(training_limit=100000)  # Set the threshold to begin training\n",
    "                ),\n",
    "                \n",
    "            ),\n",
    "            wvc.config.Configure.NamedVectors.none( \n",
    "                name=\"text_dense\",\n",
    "                vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "                    vector_cache_max_objects=100000,\n",
    "                   # quantizer=wvc.config.Reconfigure.VectorIndex.Quantizer.pq(training_limit=10000)  # Set the threshold to begin training\n",
    "                ),\n",
    "            )],\n",
    "        properties=[\n",
    "            wvc.config.Property(\n",
    "                name=\"doc_id\",\n",
    "                data_type=wvc.config.DataType.UUID,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"title_sparse\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "              wvc.config.Property(\n",
    "                name=\"document_sparse\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"title_colbert\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "           wvc.config.Property(\n",
    "                name=\"document_colbert\",\n",
    "                data_type=wvc.config.DataType.BLOB,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"title\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"text\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"url\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            )\n",
    "        ])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "823c74b126a2840e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T16:16:06.058645Z",
     "start_time": "2024-11-27T16:15:59.893417Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "docs = get_sample_docs_with_all_qrels()\n",
    "\n",
    "\n",
    "len(docs)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "662db154dede72e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T16:19:23.997637Z",
     "start_time": "2024-11-27T16:19:23.994972Z"
    }
   },
   "source": [
    "import base64\n",
    "def to_blob(obj):\n",
    "    return base64.b64encode(pickle.dumps(obj)).decode('utf-8')"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e58458bf4fd4ff63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T16:22:05.128453Z",
     "start_time": "2024-11-27T16:21:53.855290Z"
    }
   },
   "source": [
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "batches = [(i,i+10000) for i in range(0, len(docs), 10000)]\n",
    "coll = client.collections.get(\"neuclir_1_mutli_bge_m3_small\")\n",
    "\n",
    "outer_progress = tqdm(total=len(docs))\n",
    "\n",
    "for i, (start, end) in enumerate(batches):\n",
    "    if i % 50 == 0 and i != 0:\n",
    "        print(f\"Sleeping for 5 minutes to allow indexing.\")\n",
    "    batch = docs[start:end]\n",
    "    title_embeddings = model.encode(batch[\"title\"].to_list(), return_dense=True, return_sparse=False,\n",
    "                                    return_colbert_vecs=False)\n",
    "    # doc_embeddings = model.encode(batch[\"text\"].to_list(), return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "    # title_sparse_blobs = [to_blob(x) for x in title_embeddings[\"lexical_weights\"]]\n",
    "    # title_colbert_blobs = [to_blob(x) for x in title_embeddings[\"colbert_vecs\"]]\n",
    "    batch = batch.reset_index(drop=True)\n",
    "    with coll.batch.fixed_size(60, 2) as b:\n",
    "        for row in batch.itertuples(index=True):\n",
    "            #print(row)\n",
    "            b.add_object(properties={\n",
    "                \"doc_id\": row.doc_id,\n",
    "                \"title\": row.title,\n",
    "                \"text\": row.text,\n",
    "                \"url\": row.url\n",
    "                # \"title_sparse\": title_sparse_blobs[row.Index],\n",
    "                # \"title_colbert\": title_colbert_blobs[row.Index],\n",
    "            }, vector={\n",
    "                \"title_dense\": title_embeddings[\"dense_vecs\"][row.Index],\n",
    "            }, uuid=row.doc_id)\n",
    "            outer_progress.update(1)\n",
    "        if b.number_errors != 0:\n",
    "            print(f\"Found Errors: {b.number_errors}\")\n",
    "\n",
    "        b.flush()\n",
    "        sleep(10)\n",
    "\n",
    "        "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/177301 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cf6dcc971de479a9495ae8a7fa76d8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings:  12%|█▎        | 5/40 [00:08<00:56,  1.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSleeping for 5 minutes to allow indexing.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m batch \u001B[38;5;241m=\u001B[39m docs[start:end]\n\u001B[0;32m---> 13\u001B[0m title_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dense\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mreturn_colbert_vecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# doc_embeddings = model.encode(batch[\"text\"].to_list(), return_dense=True, return_sparse=True, return_colbert_vecs=False)\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# title_sparse_blobs = [to_blob(x) for x in title_embeddings[\"lexical_weights\"]]\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# title_colbert_blobs = [to_blob(x) for x in title_embeddings[\"colbert_vecs\"]]\u001B[39;00m\n\u001B[1;32m     18\u001B[0m batch \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:207\u001B[0m, in \u001B[0;36mM3Embedder.encode\u001B[0;34m(self, queries, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, **kwargs)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_sparse \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: return_sparse \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_sparse\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_colbert_vecs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: return_colbert_vecs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_colbert_vecs\n\u001B[0;32m--> 207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dense\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dense\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_colbert_vecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_colbert_vecs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py:237\u001B[0m, in \u001B[0;36mAbsEmbedder.encode\u001B[0;34m(self, sentences, batch_size, max_length, convert_to_numpy, instruction, instruction_format, **kwargs)\u001B[0m\n\u001B[1;32m    233\u001B[0m         sentences \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_detailed_instruct(instruction_format, instruction, sentence) \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m\n\u001B[1;32m    234\u001B[0m                      sentences]\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sentences, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_devices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_single_device\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m        \u001B[49m\u001B[43msentences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_devices\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_multi_process_pool(AbsEmbedder\u001B[38;5;241m.\u001B[39m_encode_multi_process_worker)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie691-ir-project-u22LL72s-py3.12/lib/python3.12/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:331\u001B[0m, in \u001B[0;36mM3Embedder.encode_single_device\u001B[0;34m(self, sentences, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, device, **kwargs)\u001B[0m\n\u001B[1;32m    323\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m    324\u001B[0m     inputs_batch,\n\u001B[1;32m    325\u001B[0m     return_dense\u001B[38;5;241m=\u001B[39mreturn_dense,\n\u001B[1;32m    326\u001B[0m     return_sparse\u001B[38;5;241m=\u001B[39mreturn_sparse,\n\u001B[1;32m    327\u001B[0m     return_colbert_vecs\u001B[38;5;241m=\u001B[39mreturn_colbert_vecs\n\u001B[1;32m    328\u001B[0m )\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dense:\n\u001B[0;32m--> 331\u001B[0m     all_dense_embeddings\u001B[38;5;241m.\u001B[39mappend(\u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdense_vecs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_sparse:\n\u001B[1;32m    334\u001B[0m     token_weights \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_vecs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "226c41cad9bfe2c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:41:02.087034Z",
     "start_time": "2024-11-27T18:41:02.077748Z"
    }
   },
   "source": [
    "\n",
    "zh = client.collections.get(\"neuclir_1_mutli_bge_m3_small\")\n",
    "aggregation = zh.aggregate.over_all(total_count=True)\n",
    "print(aggregation.total_count)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176474\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:41:00.633312Z",
     "start_time": "2024-11-27T18:41:00.614391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.cluster.nodes(\n",
    "    collection=\"neuclir_1_mutli_bge_m3_small\",\n",
    "    output=\"verbose\"\n",
    ")"
   ],
   "id": "90332a5b350cecd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(git_hash='cfdbdd0', name='node1', shards=[Shard(collection='Neuclir_1_mutli_bge_m3_small', name='3Vq0ckBvFSiX', node='node1', object_count=176474, vector_indexing_status='READY', vector_queue_length=0, compressed=False, loaded=True)], stats=Stats(object_count=176474, shard_count=1), status='HEALTHY', version='1.27.5')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "7ab8d7371f8274a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:41:13.207977Z",
     "start_time": "2024-11-27T18:41:12.187207Z"
    }
   },
   "source": [
    "from weaviate.collections.classes.grpc import MetadataQuery\n",
    "\n",
    "query_embeddings = model.encode([\"What political impact do news have on teenagers?\"], return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "response = coll.query.near_vector(near_vector=query_embeddings[\"dense_vecs\"][0], target_vector=\"title_dense\", limit=50, return_metadata=MetadataQuery(distance=True))\n",
    "\n",
    "for o in response.objects:\n",
    "    print(o.metadata.distance)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3335208296775818\n",
      "0.373696506023407\n",
      "0.3738090991973877\n",
      "0.3791655898094177\n",
      "0.380637526512146\n",
      "0.38228732347488403\n",
      "0.38613414764404297\n",
      "0.3997741937637329\n",
      "0.4023386240005493\n",
      "0.4060593843460083\n",
      "0.41171079874038696\n",
      "0.4135178327560425\n",
      "0.41399049758911133\n",
      "0.414242684841156\n",
      "0.4175682067871094\n",
      "0.42212599515914917\n",
      "0.4224163889884949\n",
      "0.4284636974334717\n",
      "0.4299066662788391\n",
      "0.4303016662597656\n",
      "0.4310094118118286\n",
      "0.43485647439956665\n",
      "0.4354240894317627\n",
      "0.43781232833862305\n",
      "0.4388442039489746\n",
      "0.43887484073638916\n",
      "0.43953585624694824\n",
      "0.44052112102508545\n",
      "0.44064629077911377\n",
      "0.4419466257095337\n",
      "0.44270193576812744\n",
      "0.4434394836425781\n",
      "0.44470804929733276\n",
      "0.44488435983657837\n",
      "0.44635099172592163\n",
      "0.4471167325973511\n",
      "0.4475809335708618\n",
      "0.4504570960998535\n",
      "0.45121055841445923\n",
      "0.4520554542541504\n",
      "0.45220035314559937\n",
      "0.45233649015426636\n",
      "0.4526246190071106\n",
      "0.45290887355804443\n",
      "0.4531651735305786\n",
      "0.4540814161300659\n",
      "0.4555288553237915\n",
      "0.4560089111328125\n",
      "0.45607990026474\n",
      "0.4564289450645447\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a829c-4ef3-4109-bffd-ede0877d812f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
